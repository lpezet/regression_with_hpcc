# 1.0 Introduction

This walkthrough on regression and modeling follows the SAS version that inspired it, written by the brilliant people from UCLA Idrea. It can be found here: [REGRESSION WITH SAS CHAPTER 1 – SIMPLE AND MULTIPLE REGRESSION](https://stats.idre.ucla.edu/sas/webbooks/reg/chapter1/regressionwith-saschapter-1-simple-and-multiple-regression/)
This article will try to follow each step taken in the original web book, try to replicate it in HPCC ECL, and present results and hopefully explain differences and where to go from there.

The emphasize will be on understanding data by running some simple analysis to find more actionable and useful means to come up with predictive models.

I assume that you already have a basic understanding of HPCC ECL, and that you have basic environment running (e.g. VirtualBox VM is fine for this exercise).

The data used here was created by randomly sampling 400 elementary schools from the California Department of Education’s API 2000 dataset. This data file contains a measure of school academic performance as well as other attributes of the elementary schools, such as, class size, enrollment, poverty, etc.
There are two versions of the data as we will see shortly, one which is *raw* (i.e. dirty) and one that has been cleaned up and used later.

The SAS *raw* file is [elemapi.sas7bdat](https://stats.idre.ucla.edu/wp-content/uploads/2016/02/elemapi-1.sas7bdat). This is the file we will be using from now on until mentioned otherwise.

## 1.0.1 Setting things up

Best is to simply clone this repository and add the containing folder to your **ECL Folders** in your ECL IDE.


## 1.0.2 Loading data

Best is to simply to use ECL Watch to upload the data file to the dropzone and spray it as CSV.
To make things faster, this ECL code will basically do the same.
Open the file `Chap1_0_GettingStarted.ecl` and submit it.


This code simply extracts the data by downloading the data files contained in this project and extracting their content.
The CSV files we're interested in here are then available in the dropzone.
Data is then loaded as "raw" data, where all variables are basically of the *STRING* type. It's best to start this way to examine the data and make sure nothing is **lost in translation** when using final field types.


# 1.1 Examining data

Before getting to the nitty gritty, it's best to understand what kind of data we are dealing with.
As far as I know, there's today no HPCC data file format that would store both the data and its description.
The following **RECORD** will have to suffice for now:

```ecl
raw_layout := RECORD
 STRING snum;       // school number
 STRING dnum;       // district number
 STRING api00;      // academic school Performan in 2000
 STRING api99;      // academic school performance in 1999
 STRING growth;     // growth 1999 to 2000
 STRING meals;      // percent free meals
 STRING ell;        // english language learners
 STRING yr_rnd;     // year round school
 STRING mobility;   // percent 1st year in school
 STRING acs_k3;     // average class size k-3
 STRING acs_46;     // average class size 4-6
 STRING not_hsg;    // parent not hsg
 STRING hsg;        // parent hsg
 STRING some_col;   // parent some college
 STRING col_grad;   // parent college grad
 STRING grad_sch;   // parent grad school
 STRING avg_ed;     // average parent ed
 STRING full;       // percent full credential
 STRING emer;       // percent emer credential
 STRING enroll;     // number of students
 STRING mealcat;    // percent free means in 3 categories
 STRING collcat;    // 
END;
```

The data file contains academic performance, average class sizes, percent of student eligible for free meals, parents education, percent of teachers with full and emergency credentials, number of enrollees, etc. per school.

Load and submit the `Chap1_1_ExaminingData.ecl` file.
Let's look at the outputs.

## 1.1.1 Sample of raw data

The output `SampleRaw` shows 100 records loaded from the `elemapi-1.csv` file, as is.
This is generated by the following action:

```
OUTPUT( oDS, NAMED('SampleRaw') );
```

Here are the first 10 records from this output:

| snum | dnum | api00 | api99 | growth | meals | ell | yr_rnd | mobility | acs_k3 | acs_46 | not_hsg | hsg | some_col | col_grad | grad_sch | avg_ed | full | emer | enroll | mealcat | collcat |
| ---- | ----- | ----- | ---- | ---- | --- | --- | ---- | ---- | ---- | --- | --- | --- | --- | --- | --- | ---- | ---- | ----- | --- | --- | --- |
| 906 | 41 | 693 | 600 | 93 | 67 | 9 | 0 | 11 | 16 | 22 | 0 | 0 | 0 | 0 | 0 |  | 76 | 24 | 247 | 2 | |
| 889 | 41 | 570 | 501 | 69 | 92 | 21 | 0 | 33 | 15 | 32 | 0 | 0 | 0 | 0 | 0 |  | 79 | 19 | 463 | 3 | | 
| 887 | 41 | 546 | 472 | 74 | 97 | 29 | 0 | 36 | 17 | 25 | 0 | 0 | 0 | 0 | 0 |  | 68 | 29 | 395 | 3 | |
| 876 | 41 | 571 | 487 | 84 | 90 | 27 | 0 | 27 | 20 | 30 | 36 | 45 | 9 | 9 | 0 | 1.909999966621399 | 87 | 11 | 418 | 3 | |
| 888 | 41 | 478 | 425 | 53 | 89 | 30 | 0 | 44 | 18 | 31 | 50 | 50 | 0 | 0 | 0 | 1.5 | 87 | 13 | 520 | 3 | |
| 4284 | 98 | 858 | 844 | 14 |  | 3 | 0 | 10 | 20 | 33 | 1 | 8 | 24 | 36 | 31 | 3.890000104904175 | 100 | 0 | 343 | 1 | |
| 4271 | 98 | 918 | 864 | 54 |  | 2 | 0 | 16 | 19 | 28 | 1 | 4 | 18 | 34 | 43 | 4.130000114440918 | 100 | 0 | 303 | 1 | |
| 2910 | 108 | 831 | 791 | 40 |  | 3 | 0 | 44 | 20 | 31 | 0 | 4 | 16 | 50 | 30 | 4.059999942779541 | 96 | 2 | 1513 | 1 | |
| 2899 | 108 | 860 | 838 | 22 |  | 6 | 0 | 10 | 20 | 30 | 2 | 9 | 15 | 42 | 33 | 3.9600000381469727 | 100 | 0 | 660 | 1 | |
| 2887 | 108 | 737 | 703 | 34 | 29 | 15 | 0 | 17 | 21 | 29 | 8 | 25 | 34 | 27 | 7 | 2.9800000190734863 | 96 | 7 | 362 | 1 | |
| ... | ..... | ..... | .... | .... | ... | ... | .... | .... | .... | ... | ... | ... | ... | ... | ... | .... | .... | ..... | ... | ... | ... | 

Listing data can be helpful to eyeball problems within it. In this sample, we can clearly see that some values are missing for *meals*., as well as fr *avg_ed*.
Thing is, it's possible we might be missing values for other variables that we can't see here because we're looking at only 100 records.

A useful way to learn about the data at hand is to get basic statistics on the variables we're interested in, like minimum value, maximum value, average, etc.
I provided a module called `Profiler` that will help with this task and shows in the next outputs.
 
## 1.1.2 All Variables
 
The `AllFields` output shows information for all variables.
This is generated from the action below:

```
Profiler.profile_fields( oProfileAllFields, oDS );
OUTPUT( oProfileAllFields, NAMED('AllFields') );
```

Which outputs the following:

| field | obs | min_value | max_value | min_value_length | max_value_length |
| -- | -- | -- | -- | -- | -- | 
| acs_46 | 397 | 20 | 50 | 2 | 2 |
| acs_k3 | 398 | -19 | 25 | 2 | 3 |
| api00 | 400 | 369 | 940 | 3 | 3 |
| api99 | 400 | 333 | 917 | 3 | 3 |
| avg_ed | 381 | 1 | 4.619999885559082 | 1 | 18 |
| ... | ... | ... | ... | ... | ... | 


It is now very easy to see now if (where) we are missing values for any of those variables.
Any variable with less than 400 observations is a variable with at least one missing value (empty).



## 1.1.3 Select Variables Values

The next output is `SelectVariablesValues` which shows some information on the values of each select variables.

The ECL code to generate such output is as follows:

```
Profiler.profile_select_fields_values( oProfileSelectFieldsValues, oDS, 'full,meals,acs_k3,api00' );
OUTPUT( oProfileSelectFieldsValues, NAMED('SelectFieldsValues') );
```

Which generates the following:

| field | value | freq |
| --- | --- | --- |
| acs_k3 | | 2 |
| acs_k3 | -19 | 1 |
| acs_k3 | -20 | 2 |
| acs_k3 | -21 | 3 |
| acs_k3 | 14 | 2 |
| ... | ... | ... |


Where the previous outputs helped understand where values were missing, this one provide a little more information on actual values by field and their frequency.
For instance, we knew already that `acs_k3` had values missing (since its ``obs = 398`` ) but here we also see some of those negative values as well.


## 1.1.4 Meals Frequency

The next output is ``MealsFrequency``. This is 

 
 
The output is as follows:
 
| field_name | number | minval | maxval | sumval | countval | mean | var | sd |
| ----- | -- | ---- | ---- | ------- | ----- | ----------------- | --------------- | ---------------- |
| acs_k3 | 10 | 0.0 | 50.0 | 11399.0 | 386.0 | 29.53108808290155 | 21.518463582915 | 4.63879979983131 |
| full | 18 | 0.0 | 59.0 | 4984.0 | 386.0 | 12.9119170984456 | 142.1269564283605 | 11.92170107108715 |
| api00 | 3 | 333.0 | 917.0 | 236741.0 | 386.0 | 613.3186528497409 | 21605.42954844422 | 146.9878551052576 |
| meals | 6 | 0.0 | 91.0 | 12307.0 | 386.0 | 31.88341968911917 | 632.1962535907005 | 25.14351315132196 |

WARNING: Again here, because of SAS Viewer export, some observations made from UCLA is off here, since we don't see like -21 for min val of acs_k3.



It is recommended to get this kind of information on all variables you're interested in using in your analysis.
The accuracy of your results greatly depends on the quality of your variables.

## 1.1.? Full

We haven't seen anything wrong with the variable ``full`` so far. But looking at....

``FullFrequency`` output:

| full | frequency | percent | cumul_frequency  | cumul_percent |
| --- | --- | --- | ---  | --- |
| 0.41999998688697815 | 1 | 0.25 | 1 | 0.25 |
| 0.44999998807907104 | 1 | 0.25 | 2 | 0.5 |
| 0.46000000834465027 | 1 | 0.25 | 3 | 0.75 |
| 0.4699999988079071 | 1 | 0.25 | 4 | 1 |
| 0.47999998927116394 | 1 | 0.25 | 5 | 1.25 |
| 0.5 | 3 | 0.75 | 8 | 2 |
| 0.5099999904632568 | 1 | 0.25 | 9 | 2.25 |
| 0.5199999809265137 | 1 | 0.25 | 10 | 2.5 |
| 0.5299999713897705 | 1 | 0.25 | 11 | 2.75 |
| 0.5400000214576721 | 1 | 0.25 | 12 | 3 |
| 0.5600000023841858 | 2 | 0.5 | 14 | 3.5 |
| 0.5699999928474426 | 2 | 0.5 | 16 | 4 |
| 0.5799999833106995 | 1 | 0.25 | 17 | 4.25 |
| 0.5899999737739563 | 3 | 0.75 | 20 | 5 |
| 0.6000000238418579 | 1 | 0.25 | 21 | 5.25 |
| 0.6100000143051147 | 4 | 1 | 25 | 6.25 |
| 0.6200000047683716 | 2 | 0.5 | 27 | 6.75 |
| 0.6299999952316284 | 1 | 0.25 | 28 | 7 |
| 0.6399999856948853 | 3 | 0.75 | 31 | 7.75 |
| 0.6499999761581421 | 3 | 0.75 | 34 | 8.5 |
| 0.6600000262260437 | 2 | 0.5 | 36 | 9 |
| 0.6700000166893005 | 6 | 1.5 | 42 | 10.5 |
| 0.6800000071525574 | 2 | 0.5 | 44 | 11 |
| 0.6899999976158142 | 3 | 0.75 | 47 | 11.75 |
| 0.699999988079071 | 1 | 0.25 | 48 | 12 |
| 0.7099999785423279 | 1 | 0.25 | 49 | 12.25 |
| 0.7200000286102295 | 2 | 0.5 | 51 | 12.75 |
| 0.7300000190734863 | 6 | 1.5 | 57 | 14.25 |
| 0.75 | 4 | 1 | 61 | 15.25 |
| 0.7599999904632568 | 2 | 0.5 | 63 | 15.75 |
| 0.7699999809265137 | 2 | 0.5 | 65 | 16.25 |
| 0.7900000214576721 | 3 | 0.75 | 68 | 17 |
| 0.800000011920929 | 5 | 1.25 | 73 | 18.25 |
| 0.8100000023841858 | 8 | 2 | 81 | 20.25 |
| 0.8199999928474426 | 2 | 0.5 | 83 | 20.75 |
| 0.8299999833106995 | 2 | 0.5 | 85 | 21.25 |
| 0.8399999737739563 | 2 | 0.5 | 87 | 21.75 |
| 0.8500000238418579 | 3 | 0.75 | 90 | 22.5 |
| 0.8600000143051147 | 2 | 0.5 | 92 | 23 |
| 0.8999999761581421 | 3 | 0.75 | 95 | 23.75 |
| 0.9200000166893005 | 1 | 0.25 | 96 | 24 |
| 0.9300000071525574 | 1 | 0.25 | 97 | 24.25 |
| 0.9399999976158142 | 2 | 0.5 | 99 | 24.75 |
| 0.949999988079071 | 2 | 0.5 | 101 | 25.25 |
| 0.9599999785423279 | 1 | 0.25 | 102 | 25.5 |
| 1 | 2 | 0.5 | 104 | 26 |
| ... | ... | ... | ...  | ... |


Let's see which district(s) these decimal values came from.

```
Profiler.freq( oFreqFullDNum, oDS( ((REAL8) full) <= 1 ), 'dnum' );
OUTPUT(oFreqFullDNum, NAMED('FullDNumFrequency') );
```

| dnum | frequency | percent | cumul_frequency | cumul_percent |
| --- | --- | --- | --- | --- |
| 401 | 104 | 100 | 104 | 100 |

All 104 observations in which ``full`` was less than or equal to one came from district *401*.
Let's see now if this accounts for all of the observations that come from district 401.

```
Profiler.freq( oFreqDNum, oDS( dnum = '401' ), 'dnum' );
OUTPUT(oFreqDNum, NAMED('DNumFrequency') );
```

| dnum | frequency | percent | cumul_frequency | cumul_percent |
| --- | --- | --- | --- | --- |
| 401 | 104 | 100 | 104 | 100 |

All of the observations from this district seem to be recorded as proportions instead of percentages.
As mentioned in the original article, this is a pretend problem that was inserted into the data for illustration purposes. If this were a real life problem, we would check with the source of the data and verify the problem.
We will make a note to fix this problem in the data as well.

Another useful graphical technique for screening your data is a scatterplot matrix.
TODO....





TODO: freq for yr_rnd
TODO: univariate acs_k3
TODO: tables acs_k3
TODO: troubleshoot negatie values for acs_k3
TODO: troubleshoot full <= 1
TODO: scatter plots!!!
TODO; regression!!!!



